{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cross_validation.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYUlkfAmiaDB"
      },
      "source": [
        "# **Cross-validation in Finance**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNu9Fojij7d"
      },
      "source": [
        "One of the purpose of ML is to learn the general structure of the data, so that we can produce predictionson future, unseen features.\n",
        "\n",
        "It is well known that when an ML algorithm is test on the same dataset as was used for training, not suprisingly, we achieve spectular results.\n",
        "\n",
        "**Cross-Validation** splits observations drawn from an IID process into two sets: the training set and the testing set.This is done as to prevent leakage from one set into the other, since that would defeat the purpose of testing on unseen data.\n",
        "\n",
        "**K-fold cross-validation** is the most popular method.This is the algorithm:\n",
        "\n",
        "\n",
        "1.   The dataset is partitioned into k subsets\n",
        "2.   For each subset:\n",
        "\n",
        "      a. The ML algorithm is trained on all subsets excluding i.\n",
        "\n",
        "      b. The fitted ML algorithm is tested on i.\n",
        "\n",
        "\n",
        "In finance, CV is used into 2 settings: model development (ex: parameter tuning) and backtesting. However, **K-fold fails is in finance** because observation cannot be assumed to be drawn from iid process. Moreover, CV leads to multiple testing and seclection bias.\n",
        "\n",
        "Let's focus one the first reason!\n",
        "\n",
        "Leakage takes place when the training set contains informations that also appears in the testing set. \n",
        "Consider a serially correlated feature X that is associated with labels Y that are formed on overlapping data:\n",
        "\n",
        "* Because of the seria correlation, $X_{t} \\approx X_{t+1}$\n",
        "* Because labels are derived from overlapping datapoints, $Y_{t} \\approx Y_{t+1}$\n",
        "\n",
        "The problem is that leakage in the presence of irrelevantfeatures will lead to false discoveries. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1c-g-tqMnge"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BemssKdLJ_d0"
      },
      "source": [
        "## Purged K-fold cross-validation:\n",
        "\n",
        "One way to reduce leakage is to purge from the training set all the observations whose labels overlapped in time with those labels included in the testing set (purging). Another is to eliminate from the training set observations that immediately follow an observation in the testing set (embargo).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ph4pE86h_x4"
      },
      "source": [
        "## This function is credited to M. Lopez de Prado\n",
        "\n",
        "def getTrainTimes(t1, testTimes):\n",
        "\n",
        "  '''\n",
        "  Given testTimes, find the times of training observations\n",
        "  t1.index: Time when the ovbservation started.\n",
        "  t1.value: Time when the observation ended.\n",
        "  testTimes: Times of testing obervations.\n",
        "  '''\n",
        "\n",
        "  trn=t1.copy(deep=True)\n",
        "  for i,j in testTimes.iteritem():\n",
        "    df0 = trn[(i <= trn.index)&(trn.index <= j)].index # train starts within test\n",
        "    df1 = trn[(i <= trn)&(trn<j)].index # train ends within test\n",
        "    df2 = trn[(trn.index <= i)&(j <= trn)].index # train envelops test\n",
        "    trn = trn.drop(df0.union(df1).union(df2))\n",
        "  \n",
        "  return trn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfwnjZNCOEpU"
      },
      "source": [
        "Note that a larger number of testing splits is not a good idea since it leads  to a greater number of overlapping observations in the training set.\n",
        "\n",
        "In many cases, purging suffices to prevent leakage. But if it doesn't, we can impose an embargo on training observations after every test set.\n",
        "\n",
        "![purging and Embargo by de Prado](https://www.quantresearch.org/Purge.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teOWQlfiOmsI"
      },
      "source": [
        "def getEmbargoTimes(times, pctEmbargo):\n",
        "  # Get embargo time for each bar\n",
        "\n",
        "  step = int(times.shape[0]*pctEmbargo)\n",
        "\n",
        "  if step == 0:\n",
        "    mbrg = pd.Series(times, index=times)\n",
        "  else:\n",
        "    mbrg = pd.Series(times[step:], index=times[:-step])\n",
        "    mbrg = mbrg.append(pd.Series(times[-1], index = times[-step:]))\n",
        "  \n",
        "  return mbrg"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBr7MvwgSLdv"
      },
      "source": [
        "## **Remarks on Sklearns cross-validation:**\n",
        "\n",
        "When working with Open-source libraries, you should always verify and adjust it to your needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6Qtem2AT6Z0"
      },
      "source": [
        "## **Example:**"
      ]
    }
  ]
}