{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "creditAnlysis.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwyVnwDknPiZ"
      },
      "source": [
        "## Credit Analysis\n",
        "Here are the different steps of this notebook:\n",
        "* Download/Fetch data\n",
        "* Features Engineering/Data Pre-processing\n",
        "* Choose/build model\n",
        "* Evaluate\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3rL62QGnKZ-"
      },
      "source": [
        "import numpy as np\n",
        "import sklearn as skl\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKpavFcMnbq7"
      },
      "source": [
        "### Data\n",
        "I found data on \"retail\"(individual) credit on Kaggle.\n",
        "You can find in the project 2 datasets. I will use only one of them.\n",
        "Retail credit analysis/modelisation is basically use by banks\n",
        "to decide whether to give a credit/mortgage to a customer or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt4R35ZLna3y"
      },
      "source": [
        "data = pd.read_csv('german_credit_data.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGBnXBxNuGJN"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnKjmlp4ta1E"
      },
      "source": [
        "data_targeted = pd.read_csv('german_credit_data_targeted.csv')\n",
        "data_targeted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybR3U-kwuJTB"
      },
      "source": [
        "data_targeted.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRmixWCWu2T4"
      },
      "source": [
        "x = data_targeted.iloc[:,:-1]\n",
        "y = data_targeted.iloc[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH-0nv-vuhx9"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
        "imputer.fit(x.iloc[:, 5:7])\n",
        "x.iloc[:, 5:7] = imputer.transform(x.iloc[:, 5:7])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN_WikbFv_zr"
      },
      "source": [
        "df = pd.DataFrame(x.iloc[:,1:])\n",
        "df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzDMMRLDytRB"
      },
      "source": [
        "cat_cols = [col for col in df.columns if df[col].dtypes == 'O']\n",
        "\n",
        "\n",
        "def one_hot_encoder(dataframe, categorical_cols, nan_as_category=True):\n",
        "     original_columns = list(dataframe.columns)\n",
        "     dataframe = pd.get_dummies(dataframe, columns=categorical_cols, dummy_na=nan_as_category, drop_first=True)\n",
        "     new_columns = [c for c in dataframe.columns if c not in original_columns]\n",
        "     return dataframe, new_columns\n",
        "\n",
        "\n",
        "df, new_cols_ohe = one_hot_encoder(df, cat_cols)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2Pk3tl-0VK2"
      },
      "source": [
        "drop_columns = ['Sex_nan', 'Housing_nan','Saving accounts_nan', 'Checking account_nan', 'Purpose_nan' ]\n",
        "df = df.drop(drop_columns, axis=1)\n"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ24-zAe1sX1"
      },
      "source": [
        "At this point, the data set could be fit to model and then be tested on an out of sample dataset.\n",
        "On some other dataset, I would have check for outliers. However, credit amount would be the only column to check for outliers in my opinion but I fear to introduce selection bias by doing that as I believe there is no norm for a credit amount."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTp8N1l05aU_"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size = 0.25, random_state = 0)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw_pxB1k5ivW"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "random_state = 10\n",
        "models = []\n",
        "models.append(('Naive Bayes Classifier', GaussianNB()))\n",
        "models.append(('Random Forest classifier',RandomForestClassifier(n_estimators = 50, criterion = 'entropy', random_state=10))) \n",
        "models.append(('Support vector Classifier',SVC(kernel = 'rbf') ))         \n",
        "               \n",
        "\n",
        "for name, model in models:\n",
        "  # Train\n",
        "  classifier = model\n",
        "  classifier.fit(X_train, y_train)\n",
        "\n",
        "  # Test\n",
        "  y_pred = classifier.predict(X_test)\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "  print(name)\n",
        "  print(cm)\n",
        "  print(accuracy_score(y_test, y_pred))\n",
        "  print(pred)\n",
        "  print('-'* 50)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}